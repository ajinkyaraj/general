{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Rediscovering_RL_Notebook_0_SOLVED.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajinkyaraj/general/blob/master/Rediscovering_RL_Notebook_0_SOLVED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv9hgPks9LhM"
      },
      "source": [
        "# Reinforcement learning with Foolsball\n",
        "- Reinforcement learning is learning to make decisions from experience.\n",
        "- Games are a good testbed for RL.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWEnCb0m9ve8"
      },
      "source": [
        "# About Foolsball\n",
        "- 5x4 playground that provides a football/foosball-like environment.\n",
        "- A controllable player:\n",
        "  - always spawned in the top-left corner\n",
        "  - displayed as 'âš½'\n",
        "  - can move North, South, East or West.\n",
        "  - can be controlled algorithmically\n",
        "- A number of **static** opponents, each represented by ðŸ‘•, that occupy certain locations on the field.\n",
        "- A goalpost ðŸ¥… that is fixed in the bottom right corner\n",
        "\n",
        "## Goals\n",
        "### Primary goal\n",
        "- We want the agent to learn to reach the goalpost \n",
        "\n",
        "### Secondary goals\n",
        "- We may want the agent to learn to be efficient in some sense, for example, take the shortest path to the goalpost. \n",
        "\n",
        "## Rules \n",
        "- Initial rules:\n",
        "    - The ball can be (tried to be) moved in four direction: \\['n','e','w',s'\\]\n",
        "    - Move the ball to an unmarked position: -1 points\n",
        "    - Move the ball to a position marked by a defender: -5 points\n",
        "    - Try to move the ball ouside the field: -1 (ball stays in the previous position)\n",
        "    - Move the ball into the goal post position: +5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlZSRf1megun"
      },
      "source": [
        "## Some variables definitions to make our enviroment look pretty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcMMYrax7MVv"
      },
      "source": [
        "agent = 'âš½'\n",
        "opponent = 'ðŸ‘•'\n",
        "goal = 'ðŸ¥…'\n",
        "\n",
        "arena = [['âš½', ' ' , 'ðŸ‘•', ' ' ],\n",
        "         [' ' , ' ' , ' ' , 'ðŸ‘•'],\n",
        "         [' ' , 'ðŸ‘•', ' ' , ' ' ],\n",
        "         [' ' , ' ' , ' ' , 'ðŸ‘•'],\n",
        "         [' ' , 'ðŸ‘•', ' ' , 'ðŸ¥…']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKaYR3mmAc6o"
      },
      "source": [
        "# Implementing an environment for the game of Foolsball\n",
        "- Text environments are simple to render in a notebook and super-fast to experiment with.\n",
        "- We want to build out own environment for two reasons:\n",
        "  - It's a great exercise in understanding the finer details, like states, actions, rewards, returns.\n",
        "  - Some of the experimentation we do requires looking under the hood of the environment, which is easier with your own implementation than a third-party environment OpenAI Gym.\n",
        "  - Environments like OpenAI Gym has a simple `step(), reset()` API that we also implement. So porting our implementation over to one of these environments should be easy (and fun)!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYOQ27nFWI9C"
      },
      "source": [
        "# Let's start!\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euv4qi0oeguq"
      },
      "source": [
        "# Step 1: Specify the APIs\n",
        "Understand why we need these APIs, their pparamters and return values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrJ1PWnreguq"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Foolsball(object):\n",
        "    \n",
        "    def __init__(self,map,agent,opponent,goal):\n",
        "        \"\"\"Spawn the world, create variables to track position of our player and its actions.\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def set_rewards(self,rewards):\n",
        "        \"\"\"Set reward/points structure\"\"\"\n",
        "        pass\n",
        "  \n",
        "    def step(self,action):\n",
        "        \"\"\"Simulate a single step in the game.\"\"\"\n",
        "        pass\n",
        "    \n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment to its initial configuration.\n",
        "        Return: the \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def render(self):\n",
        "        \"\"\"Pretty-print the environment and agent.\"\"\"\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGTQVe4begur"
      },
      "source": [
        "# Step 2: Implement a helper method __deserialize__()\n",
        "Additional helper methods `__to_state__()` and `__to_indices__()` are given."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki9emkw8egur"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Foolsball(object):\n",
        "    def __to_state__(self,row,col):\n",
        "        \"\"\"Convert from indices (row,col) to integer position.\"\"\"\n",
        "        return row*self.n_cols + col\n",
        "    \n",
        "    \n",
        "    def __to_indices__(self, state):\n",
        "        \"\"\"Convert from inteeger position to indices(row,col)\"\"\"\n",
        "        row = state // self.n_cols\n",
        "        col = state % self.n_cols\n",
        "        return row,col\n",
        "\n",
        "    def __deserialize__(self,map:list,agent:str,opponent:str, goal:str):\n",
        "        \"\"\"Convrt a string representation of a map into a 2D numpy array\n",
        "        Param map: list of lists of strings representing the player, opponents and goal.\n",
        "        Param agent: string representing the agent on the map \n",
        "        Param opponent: string representing every instance of an opponent player\n",
        "        Param goal: string representing the location of the goal on the map\n",
        "        \"\"\"\n",
        "        ## Capture dimensions and map.\n",
        "        self.n_rows = #Todo\n",
        "        self.n_cols = #Todo\n",
        "        self.n_states = #Todo\n",
        "        self.map = #Todo\n",
        "\n",
        "        ## Store string representations for printing the map, etc.\n",
        "        self.agent_repr = #Todo\n",
        "        self.opponent_repr  = #Todo\n",
        "        self.goal_repr = #Todo\n",
        "\n",
        "        ## Find initial state, the desired goal state and the state of the opponents. \n",
        "        self.init_state = None\n",
        "        self.goal_state = None\n",
        "        self.opponents_states = []\n",
        "\n",
        "        for row in range(self.n_rows):\n",
        "            for col in range(self.n_cols):\n",
        "                if map[row][col] == agent:\n",
        "                    # Store the initial state outside the map.\n",
        "                    # This helps in quickly resetting the game to the initial state and\n",
        "                    # also simplifies printing the map independent of the agent's state. \n",
        "                    self.init_state = #Todo\n",
        "                    self.map[row,col] = ' ' \n",
        "\n",
        "                    elif map[row][col] == opponent:\n",
        "                      #Todo\n",
        "\n",
        "                    elif map[row][col] == goal:\n",
        "                      #Todo\n",
        "\n",
        "        assert self.init_state is not None, print(f\"Map {map} does not specify an agent {agent} location\")\n",
        "        assert self.goal_state is not None,  print(f\"Map {map} does not specify a goal {goal} location\")\n",
        "        assert self.opponents_states,  print(f\"Map {map} does not specify any opponents {opponent} location\")\n",
        "\n",
        "        return self.init_state\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ByTcNKPegus"
      },
      "source": [
        "## Test that the code is working\n",
        "foolsball = Foolsball()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB3RmU0vegut"
      },
      "source": [
        "# Step 3: Complete the __init__() method\n",
        "- Understand how `set_reward()` is being used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k0d1zDzegut"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Foolsball(object):\n",
        "    def __to_state__(self,row,col):\n",
        "        \"\"\"Convert from indices (row,col) to integer position.\"\"\"\n",
        "        return row*self.n_cols + col\n",
        "    \n",
        "    \n",
        "    def __to_indices__(self, state):\n",
        "        \"\"\"Convert from inteeger position to indices(row,col)\"\"\"\n",
        "        row = state // self.n_cols\n",
        "        col = state % self.n_cols\n",
        "        return row,col\n",
        "\n",
        "    def __deserialize__(self,map:list,agent:str,opponent:str, goal:str):\n",
        "        \"\"\"Convrt a string representation of a map into a 2D numpy array\n",
        "        Param map: list of lists of strings representing the player, opponents and goal.\n",
        "        Param agent: string representing the agent on the map \n",
        "        Param opponent: string representing every instance of an opponent player\n",
        "        Param goal: string representing the location of the goal on the map\n",
        "        \"\"\"\n",
        "        ## Capture dimensions and map.\n",
        "        self.n_rows = len(map)\n",
        "        self.n_cols = len(map[0])\n",
        "        self.n_states = self.n_rows * self.n_cols\n",
        "        self.map = np.asarray(map)\n",
        "\n",
        "        ## Store string representations for printing the map, etc.\n",
        "        self.agent_repr = agent\n",
        "        self.opponent_repr  = opponent\n",
        "        self.goal_repr = goal\n",
        "\n",
        "        ## Find initial state, the desired goal state and the state of the opponents. \n",
        "        self.init_state = None\n",
        "        self.goal_state = None\n",
        "        self.opponents_states = []\n",
        "\n",
        "        for row in range(self.n_rows):\n",
        "            for col in range(self.n_cols):\n",
        "                if map[row][col] == agent:\n",
        "                    # Store the initial state outside the map.\n",
        "                    # This helps in quickly resetting the game to the initial state and\n",
        "                    # also simplifies printing the map independent of the agent's state. \n",
        "                    self.init_state = self.__to_state__(row,col)\n",
        "                    self.map[row,col] = ' ' \n",
        "\n",
        "                elif map[row][col] == opponent:\n",
        "                    self.opponents_states.append(self.__to_state__(row,col))\n",
        "\n",
        "                elif map[row][col] == goal:\n",
        "                    self.goal_state = self.__to_state__(row,col)\n",
        "\n",
        "        assert self.init_state is not None, print(f\"Map {map} does not specify an agent {agent} location\")\n",
        "        assert self.goal_state is not None,  print(f\"Map {map} does not specify a goal {goal} location\")\n",
        "        assert self.opponents_states,  print(f\"Map {map} does not specify any opponents {opponent} location\")\n",
        "\n",
        "        return self.init_state\n",
        "    \n",
        "    \n",
        "    def __init__(self,map,agent,opponent,goal):\n",
        "        \"\"\"Spawn the world, create variables to track state and actions.\"\"\"\n",
        "        # We just need to track the position of the agent (the ball)\n",
        "        # Everything else is static and so a potential algorithm doesn't \n",
        "        # have to look at it. The variable `done` flags the end of the game.\n",
        "        self.state = #Todo\n",
        "        self.done = #Todo\n",
        "        self.actions = #Todo\n",
        "\n",
        "        # Set up the rewards\n",
        "        self.default_rewards = {'unmarked':-1, 'opponent':-5, 'outside':-1, 'goal':+5}\n",
        "        self.set_rewards(self.default_rewards)\n",
        "        \n",
        "    def set_rewards(self,rewards):\n",
        "        if not self.state == self.init_state:\n",
        "            print('Warning: Setting reward while not in initial state! You may want to call reset() first.')\n",
        "        for key in self.default_rewards:\n",
        "            assert key in rewards, print(f'Key {key} missing from reward.') \n",
        "            self.rewards = rewards\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtRbza9Peguu"
      },
      "source": [
        "## Test that the code works\n",
        "foolsball = Foolsball(arena, agent, opponent, goal)\n",
        "foolsball.set_rewards({'unmarked':0, 'opponent':0, 'outside':0, 'goal':0})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbqqT_eseguv"
      },
      "source": [
        "# Step 4: Implement the reset() method "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS0h1AYBeguv"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Foolsball(object):\n",
        "    def __to_state__(self,row,col):\n",
        "        \"\"\"Convert from indices (row,col) to integer position.\"\"\"\n",
        "        return row*self.n_cols + col\n",
        "    \n",
        "    \n",
        "    def __to_indices__(self, state):\n",
        "        \"\"\"Convert from inteeger position to indices(row,col)\"\"\"\n",
        "        row = state // self.n_cols\n",
        "        col = state % self.n_cols\n",
        "        return row,col\n",
        "\n",
        "    def __deserialize__(self,map:list,agent:str,opponent:str, goal:str):\n",
        "        \"\"\"Convrt a string representation of a map into a 2D numpy array\n",
        "        Param map: list of lists of strings representing the player, opponents and goal.\n",
        "        Param agent: string representing the agent on the map \n",
        "        Param opponent: string representing every instance of an opponent player\n",
        "        Param goal: string representing the location of the goal on the map\n",
        "        \"\"\"\n",
        "        ## Capture dimensions and map.\n",
        "        self.n_rows = len(map)\n",
        "        self.n_cols = len(map[0])\n",
        "        self.n_states = self.n_rows * self.n_cols\n",
        "        self.map = np.asarray(map)\n",
        "\n",
        "        ## Store string representations for printing the map, etc.\n",
        "        self.agent_repr = agent\n",
        "        self.opponent_repr  = opponent\n",
        "        self.goal_repr = goal\n",
        "\n",
        "        ## Find initial state, the desired goal state and the state of the opponents. \n",
        "        self.init_state = None\n",
        "        self.goal_state = None\n",
        "        self.opponents_states = []\n",
        "\n",
        "        for row in range(self.n_rows):\n",
        "            for col in range(self.n_cols):\n",
        "                if map[row][col] == agent:\n",
        "                    # Store the initial state outside the map.\n",
        "                    # This helps in quickly resetting the game to the initial state and\n",
        "                    # also simplifies printing the map independent of the agent's state. \n",
        "                    self.init_state = self.__to_state__(row,col)\n",
        "                    self.map[row,col] = ' ' \n",
        "\n",
        "                elif map[row][col] == opponent:\n",
        "                    self.opponents_states.append(self.__to_state__(row,col))\n",
        "\n",
        "                elif map[row][col] == goal:\n",
        "                    self.goal_state = self.__to_state__(row,col)\n",
        "\n",
        "        assert self.init_state is not None, print(f\"Map {map} does not specify an agent {agent} location\")\n",
        "        assert self.goal_state is not None,  print(f\"Map {map} does not specify a goal {goal} location\")\n",
        "        assert self.opponents_states,  print(f\"Map {map} does not specify any opponents {opponent} location\")\n",
        "\n",
        "        return self.init_state\n",
        "    \n",
        "    \n",
        "    def __init__(self,map,agent,opponent,goal):\n",
        "        \"\"\"Spawn the world, create variables to track state and actions.\"\"\"\n",
        "        # We just need to track the location of the agent (the ball)\n",
        "        # Everything else is static and so a potential algorithm doesn't \n",
        "        # have to look at it. The variable `done` flags terminal states.\n",
        "        self.state = self.__deserialize__(map,agent,opponent,goal)\n",
        "        self.done = False\n",
        "        self.actions = ['n','e','w','s']\n",
        "\n",
        "        # Set up the rewards\n",
        "        self.default_rewards = {'unmarked':-1, 'opponent':-5, 'outside':-1, 'goal':+5}\n",
        "        self.set_rewards(self.default_rewards)\n",
        "        \n",
        "    def set_rewards(self,rewards):\n",
        "        if not self.state == self.init_state:\n",
        "            print('Warning: Setting reward while not in initial state! You may want to call reset() first.')\n",
        "        for key in self.default_rewards:\n",
        "            assert key in rewards, print(f'Key {key} missing from reward.') \n",
        "            self.rewards = rewards\n",
        "            \n",
        "            \n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment to its initial state.\"\"\"\n",
        "        # There's really just two things we need to reset: the state, which should\n",
        "        # be reset to the initial state, and the `done` flag which should be \n",
        "        # cleared to signal that the game has not terminated. \n",
        "        self.state = #Todo\n",
        "        self.done  = #Todo\n",
        "        return self.state\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6LprJ4ueguw"
      },
      "source": [
        "## Test that the code works\n",
        "foolsball = Foolsball(arena, agent, opponent, goal)\n",
        "foolsball.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYLEgrQgegux"
      },
      "source": [
        "# Step 5: Implement the helper method __get_next_state_on_action__()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeE4iFcregux"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Foolsball(object):\n",
        "    def __to_state__(self,row,col):\n",
        "        \"\"\"Convert from indices (row,col) to integer position.\"\"\"\n",
        "        return row*self.n_cols + col\n",
        "    \n",
        "    \n",
        "    def __to_indices__(self, state):\n",
        "        \"\"\"Convert from inteeger position to indices(row,col)\"\"\"\n",
        "        row = state // self.n_cols\n",
        "        col = state % self.n_cols\n",
        "        return row,col\n",
        "\n",
        "    def __deserialize__(self,map:list,agent:str,opponent:str, goal:str):\n",
        "        \"\"\"Convrt a string representation of a map into a 2D numpy array\n",
        "        Param map: list of lists of strings representing the player, opponents and goal.\n",
        "        Param agent: string representing the agent on the map \n",
        "        Param opponent: string representing every instance of an opponent player\n",
        "        Param goal: string representing the location of the goal on the map\n",
        "        \"\"\"\n",
        "        ## Capture dimensions and map.\n",
        "        self.n_rows = len(map)\n",
        "        self.n_cols = len(map[0])\n",
        "        self.n_states = self.n_rows * self.n_cols\n",
        "        self.map = np.asarray(map)\n",
        "\n",
        "        ## Store string representations for printing the map, etc.\n",
        "        self.agent_repr = agent\n",
        "        self.opponent_repr  = opponent\n",
        "        self.goal_repr = goal\n",
        "\n",
        "        ## Find initial state, the desired goal state and the state of the opponents. \n",
        "        self.init_state = None\n",
        "        self.goal_state = None\n",
        "        self.opponents_states = []\n",
        "\n",
        "        for row in range(self.n_rows):\n",
        "            for col in range(self.n_cols):\n",
        "                if map[row][col] == agent:\n",
        "                    # Store the initial state outside the map.\n",
        "                    # This helps in quickly resetting the game to the initial state and\n",
        "                    # also simplifies printing the map independent of the agent's state. \n",
        "                    self.init_state = self.__to_state__(row,col)\n",
        "                    self.map[row,col] = ' ' \n",
        "\n",
        "                elif map[row][col] == opponent:\n",
        "                    self.opponents_states.append(self.__to_state__(row,col))\n",
        "\n",
        "                elif map[row][col] == goal:\n",
        "                    self.goal_state = self.__to_state__(row,col)\n",
        "\n",
        "        assert self.init_state is not None, print(f\"Map {map} does not specify an agent {agent} location\")\n",
        "        assert self.goal_state is not None,  print(f\"Map {map} does not specify a goal {goal} location\")\n",
        "        assert self.opponents_states,  print(f\"Map {map} does not specify any opponents {opponent} location\")\n",
        "\n",
        "        return self.init_state\n",
        "    \n",
        "    \n",
        "    def __init__(self,map,agent,opponent,goal):\n",
        "        \"\"\"Spawn the world, create variables to track state and actions.\"\"\"\n",
        "        # We just need to track the location of the agent (the ball)\n",
        "        # Everything else is static and so a potential algorithm doesn't \n",
        "        # have to look at it. The variable `done` flags terminal states.\n",
        "        self.state = self.__deserialize__(map,agent,opponent,goal)\n",
        "        self.done = False\n",
        "        self.actions = ['n','e','w','s']\n",
        "\n",
        "        # Set up the rewards\n",
        "        self.default_rewards = {'unmarked':-1, 'opponent':-5, 'outside':-1, 'goal':+5}\n",
        "        self.set_rewards(self.default_rewards)\n",
        "        \n",
        "    def set_rewards(self,rewards):\n",
        "        if not self.state == self.init_state:\n",
        "            print('Warning: Setting reward while not in initial state! You may want to call reset() first.')\n",
        "        for key in self.default_rewards:\n",
        "            assert key in rewards, print(f'Key {key} missing from reward.') \n",
        "        self.rewards = rewards\n",
        "            \n",
        "            \n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment to its initial state.\"\"\"\n",
        "        # There's really just two things we need to reset: the state, which should\n",
        "        # be reset to the initial state, and the `done` flag which should be \n",
        "        # cleared to signal that we are not in a terminal state anymore, even if we \n",
        "        # were earlier. \n",
        "        self.state = self.init_state\n",
        "        self.done  = False\n",
        "        return self.state\n",
        "    \n",
        "    def __get_next_state_on_action__(self,state,action):\n",
        "        \"\"\"Return next state based on current state and action.\"\"\"\n",
        "        row, col = self.__to_indices__(state)\n",
        "        action_to_index_delta = {'n':[-1,0], 'e':[0,+1], 'w':[0,-1], 's':[+1,0]}\n",
        "\n",
        "        row_delta, col_delta = action_to_index_delta[action]\n",
        "        new_row , new_col = row+row_delta, col+col_delta\n",
        "\n",
        "        ## Return current state if next state is invalid\n",
        "        if #Todo\n",
        "          return state  \n",
        "\n",
        "        ## Construct state from new row and col and return it.    \n",
        "        return #Todo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmc-BSCNeguy"
      },
      "source": [
        "# Test that the code works\n",
        "foolsball = Foolsball(arena, agent, opponent, goal)\n",
        "foolsball.reset()\n",
        "foolsball.__get_next_state_on_action__(0,'e')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj3OApTgeguy"
      },
      "source": [
        "# Step 6: Implement the helper method: __get_reward_for_transition__()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVfSvT02eguy"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Foolsball(object):\n",
        "    def __to_state__(self,row,col):\n",
        "        \"\"\"Convert from indices (row,col) to integer position.\"\"\"\n",
        "        return row*self.n_cols + col\n",
        "    \n",
        "    \n",
        "    def __to_indices__(self, state):\n",
        "        \"\"\"Convert from inteeger position to indices(row,col)\"\"\"\n",
        "        row = state // self.n_cols\n",
        "        col = state % self.n_cols\n",
        "        return row,col\n",
        "\n",
        "    def __deserialize__(self,map:list,agent:str,opponent:str, goal:str):\n",
        "        \"\"\"Convrt a string representation of a map into a 2D numpy array\n",
        "        Param map: list of lists of strings representing the player, opponents and goal.\n",
        "        Param agent: string representing the agent on the map \n",
        "        Param opponent: string representing every instance of an opponent player\n",
        "        Param goal: string representing the location of the goal on the map\n",
        "        \"\"\"\n",
        "        ## Capture dimensions and map.\n",
        "        self.n_rows = len(map)\n",
        "        self.n_cols = len(map[0])\n",
        "        self.n_states = self.n_rows * self.n_cols\n",
        "        self.map = np.asarray(map)\n",
        "\n",
        "        ## Store string representations for printing the map, etc.\n",
        "        self.agent_repr = agent\n",
        "        self.opponent_repr  = opponent\n",
        "        self.goal_repr = goal\n",
        "\n",
        "        ## Find initial state, the desired goal state and the state of the opponents. \n",
        "        self.init_state = None\n",
        "        self.goal_state = None\n",
        "        self.opponents_states = []\n",
        "\n",
        "        for row in range(self.n_rows):\n",
        "            for col in range(self.n_cols):\n",
        "                if map[row][col] == agent:\n",
        "                    # Store the initial state outside the map.\n",
        "                    # This helps in quickly resetting the game to the initial state and\n",
        "                    # also simplifies printing the map independent of the agent's state. \n",
        "                    self.init_state = self.__to_state__(row,col)\n",
        "                    self.map[row,col] = ' ' \n",
        "\n",
        "                elif map[row][col] == opponent:\n",
        "                    self.opponents_states.append(self.__to_state__(row,col))\n",
        "\n",
        "                elif map[row][col] == goal:\n",
        "                    self.goal_state = self.__to_state__(row,col)\n",
        "\n",
        "        assert self.init_state is not None, print(f\"Map {map} does not specify an agent {agent} location\")\n",
        "        assert self.goal_state is not None,  print(f\"Map {map} does not specify a goal {goal} location\")\n",
        "        assert self.opponents_states,  print(f\"Map {map} does not specify any opponents {opponent} location\")\n",
        "\n",
        "        return self.init_state\n",
        "    \n",
        "    \n",
        "    def __init__(self,map,agent,opponent,goal):\n",
        "        \"\"\"Spawn the world, create variables to track state and actions.\"\"\"\n",
        "        # We just need to track the location of the agent (the ball)\n",
        "        # Everything else is static and so a potential algorithm doesn't \n",
        "        # have to look at it. The variable `done` flags terminal states.\n",
        "        self.state = self.__deserialize__(map,agent,opponent,goal)\n",
        "        self.done = False\n",
        "        self.actions = ['n','e','w','s']\n",
        "\n",
        "        # Set up the rewards\n",
        "        self.default_rewards = {'unmarked':-1, 'opponent':-5, 'outside':-1, 'goal':+5}\n",
        "        self.set_rewards(self.default_rewards)\n",
        "        \n",
        "    def set_rewards(self,rewards):\n",
        "        if not self.state == self.init_state:\n",
        "            print('Warning: Setting reward while not in initial state! You may want to call reset() first.')\n",
        "        for key in self.default_rewards:\n",
        "            assert key in rewards, print(f'Key {key} missing from reward.') \n",
        "        self.rewards = rewards\n",
        "            \n",
        "            \n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment to its initial state.\"\"\"\n",
        "        # There's really just two things we need to reset: the state, which should\n",
        "        # be reset to the initial state, and the `done` flag which should be \n",
        "        # cleared to signal that we are not in a terminal state anymore, even if we \n",
        "        # were earlier. \n",
        "        self.state = self.init_state\n",
        "        self.done  = False\n",
        "        return self.state\n",
        "    \n",
        "    def __get_next_state_on_action__(self,state,action):\n",
        "        \"\"\"Return next state based on current state and action.\"\"\"\n",
        "        row, col = self.__to_indices__(state)\n",
        "        action_to_index_delta = {'n':[-1,0], 'e':[0,+1], 'w':[0,-1], 's':[+1,0]}\n",
        "\n",
        "        row_delta, col_delta = action_to_index_delta[action]\n",
        "        new_row , new_col = row+row_delta, col+col_delta\n",
        "\n",
        "        ## Return current state if next state is invalid\n",
        "        if not(0<=new_row<self.n_rows) or not(0<=new_col<self.n_cols):\n",
        "            return state  \n",
        "\n",
        "        ## Construct state from new row and col and return it.    \n",
        "        return self.__to_state__(new_row, new_col)    \n",
        "    \n",
        "  \n",
        "    def __get_reward_for_transition__(self,state,next_state):\n",
        "        \"\"\" Return the reward based on the transition from current state to next state. \"\"\"\n",
        "        ## Transition rejected due to illegal action (move)\n",
        "        if next_state == state:\n",
        "            reward = #Todo\n",
        "\n",
        "        ## Goal!\n",
        "        elif next_state == self.goal_state:\n",
        "            reward = #Todo\n",
        "\n",
        "        ## Ran into opponent. \n",
        "        elif next_state in self.opponents_states:\n",
        "            reward = self.rewards['opponent']\n",
        "\n",
        "        ## Made a safe and valid move.   \n",
        "        else:\n",
        "            reward = #Todo\n",
        "\n",
        "        return reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiWOKzeoeguz"
      },
      "source": [
        "# Test that the code works\n",
        "foolsball = Foolsball(arena, agent, opponent, goal)\n",
        "foolsball.reset()\n",
        "foolsball.__get_next_state_on_action__(0,'e')\n",
        "foolsball.__get_reward_for_transition__(1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adFKTMRveguz"
      },
      "source": [
        "# Step 7: Implement the step() method\n",
        "- Use the __is_terminal_state__() method provided to test if the game has ended. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKcOYLPFegu0"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Foolsball(object):\n",
        "    def __to_state__(self,row,col):\n",
        "        \"\"\"Convert from indices (row,col) to integer position.\"\"\"\n",
        "        return row*self.n_cols + col\n",
        "    \n",
        "    \n",
        "    def __to_indices__(self, state):\n",
        "        \"\"\"Convert from inteeger position to indices(row,col)\"\"\"\n",
        "        row = state // self.n_cols\n",
        "        col = state % self.n_cols\n",
        "        return row,col\n",
        "\n",
        "    def __deserialize__(self,map:list,agent:str,opponent:str, goal:str):\n",
        "        \"\"\"Convrt a string representation of a map into a 2D numpy array\n",
        "        Param map: list of lists of strings representing the player, opponents and goal.\n",
        "        Param agent: string representing the agent on the map \n",
        "        Param opponent: string representing every instance of an opponent player\n",
        "        Param goal: string representing the location of the goal on the map\n",
        "        \"\"\"\n",
        "        ## Capture dimensions and map.\n",
        "        self.n_rows = len(map)\n",
        "        self.n_cols = len(map[0])\n",
        "        self.n_states = self.n_rows * self.n_cols\n",
        "        self.map = np.asarray(map)\n",
        "\n",
        "        ## Store string representations for printing the map, etc.\n",
        "        self.agent_repr = agent\n",
        "        self.opponent_repr  = opponent\n",
        "        self.goal_repr = goal\n",
        "\n",
        "        ## Find initial state, the desired goal state and the state of the opponents. \n",
        "        self.init_state = None\n",
        "        self.goal_state = None\n",
        "        self.opponents_states = []\n",
        "\n",
        "        for row in range(self.n_rows):\n",
        "            for col in range(self.n_cols):\n",
        "                if map[row][col] == agent:\n",
        "                    # Store the initial state outside the map.\n",
        "                    # This helps in quickly resetting the game to the initial state and\n",
        "                    # also simplifies printing the map independent of the agent's state. \n",
        "                    self.init_state = self.__to_state__(row,col)\n",
        "                    self.map[row,col] = ' ' \n",
        "\n",
        "                elif map[row][col] == opponent:\n",
        "                    self.opponents_states.append(self.__to_state__(row,col))\n",
        "\n",
        "                elif map[row][col] == goal:\n",
        "                    self.goal_state = self.__to_state__(row,col)\n",
        "\n",
        "        assert self.init_state is not None, print(f\"Map {map} does not specify an agent {agent} location\")\n",
        "        assert self.goal_state is not None,  print(f\"Map {map} does not specify a goal {goal} location\")\n",
        "        assert self.opponents_states,  print(f\"Map {map} does not specify any opponents {opponent} location\")\n",
        "\n",
        "        return self.init_state\n",
        "    \n",
        "    \n",
        "    def __init__(self,map,agent,opponent,goal):\n",
        "        \"\"\"Spawn the world, create variables to track state and actions.\"\"\"\n",
        "        # We just need to track the location of the agent (the ball)\n",
        "        # Everything else is static and so a potential algorithm doesn't \n",
        "        # have to look at it. The variable `done` flags terminal states.\n",
        "        self.state = self.__deserialize__(map,agent,opponent,goal)\n",
        "        self.done = False\n",
        "        self.actions = ['n','e','w','s']\n",
        "\n",
        "        # Set up the rewards\n",
        "        self.default_rewards = {'unmarked':-1, 'opponent':-5, 'outside':-1, 'goal':+5}\n",
        "        self.set_rewards(self.default_rewards)\n",
        "        \n",
        "    def set_rewards(self,rewards):\n",
        "        if not self.state == self.init_state:\n",
        "            print('Warning: Setting reward while not in initial state! You may want to call reset() first.')\n",
        "        for key in self.default_rewards:\n",
        "            assert key in rewards, print(f'Key {key} missing from reward.') \n",
        "        self.rewards = rewards\n",
        "            \n",
        "            \n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment to its initial state.\"\"\"\n",
        "        # There's really just two things we need to reset: the state, which should\n",
        "        # be reset to the initial state, and the `done` flag which should be \n",
        "        # cleared to signal that we are not in a terminal state anymore, even if we \n",
        "        # were earlier. \n",
        "        self.state = self.init_state\n",
        "        self.done  = False\n",
        "        return self.state\n",
        "    \n",
        "    def __get_next_state_on_action__(self,state,action):\n",
        "        \"\"\"Return next state based on current state and action.\"\"\"\n",
        "        row, col = self.__to_indices__(state)\n",
        "        action_to_index_delta = {'n':[-1,0], 'e':[0,+1], 'w':[0,-1], 's':[+1,0]}\n",
        "\n",
        "        row_delta, col_delta = action_to_index_delta[action]\n",
        "        new_row , new_col = row+row_delta, col+col_delta\n",
        "\n",
        "        ## Return current state if next state is invalid\n",
        "        if not(0<=new_row<self.n_rows) or not(0<=new_col<self.n_cols):\n",
        "            return state  \n",
        "\n",
        "        ## Construct state from new row and col and return it.    \n",
        "        return self.__to_state__(new_row, new_col)    \n",
        "    \n",
        "  \n",
        "    def __get_reward_for_transition__(self,state,next_state):\n",
        "        \"\"\" Return the reward based on the transition from current state to next state. \"\"\"\n",
        "        ## Transition rejected due to illegal action (move)\n",
        "        if next_state == state:\n",
        "            reward = self.rewards['outside']\n",
        "\n",
        "        ## Goal!\n",
        "        elif next_state == self.goal_state:\n",
        "            reward = self.rewards['goal']\n",
        "\n",
        "        ## Ran into opponent. \n",
        "        elif next_state in self.opponents_states:\n",
        "            reward = self.rewards['opponent']\n",
        "\n",
        "        ## Made a safe and valid move.   \n",
        "        else:\n",
        "            reward = self.rewards['unmarked']\n",
        "\n",
        "        return reward    \n",
        "    \n",
        "    \n",
        "    def __is_terminal_state__(self, state):\n",
        "        return (state == self.goal_state) or (state in self.opponents_states) \n",
        "    \n",
        "      \n",
        "    def step(self,action):\n",
        "        \"\"\"Simulate state transition based on current state and action received.\"\"\"\n",
        "        assert not self.done, \\\n",
        "        print(f'You cannot call step() in a terminal state({self.state}). Check the \"done\" flag before calling step() to avoid this.')\n",
        "        next_state = #Todo: Get next state for this (state,action) pair.\n",
        "\n",
        "        reward = #Todo: Get the reward for the state -> next_state transition.\n",
        "\n",
        "        done = #Todo: Set the flag if we are in a terminal state.\n",
        "\n",
        "        self.state, self.done = next_state, done\n",
        "\n",
        "        return next_state, reward, done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mZ7TD3aegu0"
      },
      "source": [
        "# Test that the code works\n",
        "foolsball = Foolsball(arena, agent, opponent, goal)\n",
        "foolsball.reset()\n",
        "foolsball.__get_next_state_on_action__(0,'e')\n",
        "foolsball.__get_reward_for_transition__(1,2)\n",
        "foolsball.step('e')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDNf2128egu1"
      },
      "source": [
        "# Step 8: Copy the render() method from [here](https://gist.github.com/farhan-knowchow/ce55ed08fc5ed325e421a5ab66f83ab9) into your class to complete the implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJjrL3-Qegu1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Foolsball(object):\n",
        "    def __to_state__(self,row,col):\n",
        "        \"\"\"Convert from indices (row,col) to integer position.\"\"\"\n",
        "        return row*self.n_cols + col\n",
        "    \n",
        "    \n",
        "    def __to_indices__(self, state):\n",
        "        \"\"\"Convert from inteeger position to indices(row,col)\"\"\"\n",
        "        row = state // self.n_cols\n",
        "        col = state % self.n_cols\n",
        "        return row,col\n",
        "\n",
        "    def __deserialize__(self,map:list,agent:str,opponent:str, goal:str):\n",
        "        \"\"\"Convrt a string representation of a map into a 2D numpy array\n",
        "        Param map: list of lists of strings representing the player, opponents and goal.\n",
        "        Param agent: string representing the agent on the map \n",
        "        Param opponent: string representing every instance of an opponent player\n",
        "        Param goal: string representing the location of the goal on the map\n",
        "        \"\"\"\n",
        "        ## Capture dimensions and map.\n",
        "        self.n_rows = len(map)\n",
        "        self.n_cols = len(map[0])\n",
        "        self.n_states = self.n_rows * self.n_cols\n",
        "        self.map = np.asarray(map)\n",
        "\n",
        "        ## Store string representations for printing the map, etc.\n",
        "        self.agent_repr = agent\n",
        "        self.opponent_repr  = opponent\n",
        "        self.goal_repr = goal\n",
        "\n",
        "        ## Find initial state, the desired goal state and the state of the opponents. \n",
        "        self.init_state = None\n",
        "        self.goal_state = None\n",
        "        self.opponents_states = []\n",
        "\n",
        "        for row in range(self.n_rows):\n",
        "            for col in range(self.n_cols):\n",
        "                if map[row][col] == agent:\n",
        "                    # Store the initial state outside the map.\n",
        "                    # This helps in quickly resetting the game to the initial state and\n",
        "                    # also simplifies printing the map independent of the agent's state. \n",
        "                    self.init_state = self.__to_state__(row,col)\n",
        "                    self.map[row,col] = ' ' \n",
        "\n",
        "                elif map[row][col] == opponent:\n",
        "                    self.opponents_states.append(self.__to_state__(row,col))\n",
        "\n",
        "                elif map[row][col] == goal:\n",
        "                    self.goal_state = self.__to_state__(row,col)\n",
        "\n",
        "        assert self.init_state is not None, print(f\"Map {map} does not specify an agent {agent} location\")\n",
        "        assert self.goal_state is not None,  print(f\"Map {map} does not specify a goal {goal} location\")\n",
        "        assert self.opponents_states,  print(f\"Map {map} does not specify any opponents {opponent} location\")\n",
        "\n",
        "        return self.init_state\n",
        "    \n",
        "    \n",
        "    def __init__(self,map,agent,opponent,goal):\n",
        "        \"\"\"Spawn the world, create variables to track state and actions.\"\"\"\n",
        "        # We just need to track the location of the agent (the ball)\n",
        "        # Everything else is static and so a potential algorithm doesn't \n",
        "        # have to look at it. The variable `done` flags terminal states.\n",
        "        self.state = self.__deserialize__(map,agent,opponent,goal)\n",
        "        self.done = False\n",
        "        self.actions = ['n','e','w','s']\n",
        "\n",
        "        # Set up the rewards\n",
        "        self.default_rewards = {'unmarked':-1, 'opponent':-5, 'outside':-1, 'goal':+5}\n",
        "        self.set_rewards(self.default_rewards)\n",
        "        \n",
        "    def set_rewards(self,rewards):\n",
        "        if not self.state == self.init_state:\n",
        "            print('Warning: Setting reward while not in initial state! You may want to call reset() first.')\n",
        "        for key in self.default_rewards:\n",
        "            assert key in rewards, print(f'Key {key} missing from reward.') \n",
        "        self.rewards = rewards\n",
        "            \n",
        "            \n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment to its initial state.\"\"\"\n",
        "        # There's really just two things we need to reset: the state, which should\n",
        "        # be reset to the initial state, and the `done` flag which should be \n",
        "        # cleared to signal that we are not in a terminal state anymore, even if we \n",
        "        # were earlier. \n",
        "        self.state = self.init_state\n",
        "        self.done  = False\n",
        "        return self.state\n",
        "    \n",
        "    def __get_next_state_on_action__(self,state,action):\n",
        "        \"\"\"Return next state based on current state and action.\"\"\"\n",
        "        row, col = self.__to_indices__(state)\n",
        "        action_to_index_delta = {'n':[-1,0], 'e':[0,+1], 'w':[0,-1], 's':[+1,0]}\n",
        "\n",
        "        row_delta, col_delta = action_to_index_delta[action]\n",
        "        new_row , new_col = row+row_delta, col+col_delta\n",
        "\n",
        "        ## Return current state if next state is invalid\n",
        "        if not(0<=new_row<self.n_rows) or not(0<=new_col<self.n_cols):\n",
        "            return state  \n",
        "\n",
        "        ## Construct state from new row and col and return it.    \n",
        "        return self.__to_state__(new_row, new_col)    \n",
        "    \n",
        "  \n",
        "    def __get_reward_for_transition__(self,state,next_state):\n",
        "        \"\"\" Return the reward based on the transition from current state to next state. \"\"\"\n",
        "        ## Transition rejected due to illegal action (move)\n",
        "        if next_state == state:\n",
        "            reward = self.rewards['outside']\n",
        "\n",
        "        ## Goal!\n",
        "        elif next_state == self.goal_state:\n",
        "            reward = self.rewards['goal']\n",
        "\n",
        "        ## Ran into opponent. \n",
        "        elif next_state in self.opponents_states:\n",
        "            reward = self.rewards['opponent']\n",
        "\n",
        "        ## Made a safe and valid move.   \n",
        "        else:\n",
        "            reward = self.rewards['unmarked']\n",
        "\n",
        "        return reward    \n",
        "    \n",
        "    \n",
        "    def __is_terminal_state__(self, state):\n",
        "        return (state == self.goal_state) or (state in self.opponents_states) \n",
        "    \n",
        "      \n",
        "    def step(self,action):\n",
        "        \"\"\"Simulate state transition based on current state and action received.\"\"\"\n",
        "        assert not self.done, \\\n",
        "        print(f'You cannot call step() in a terminal state({self.state}). Check the \"done\" flag before calling step() to avoid this.')\n",
        "        next_state = self.__get_next_state_on_action__(self.state, action)\n",
        "\n",
        "        reward = self.__get_reward_for_transition__(self.state, next_state)\n",
        "\n",
        "        done = self.__is_terminal_state__(next_state)\n",
        "\n",
        "        self.state, self.done = next_state, done\n",
        "\n",
        "        return next_state, reward, done\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWXHSX9IWm9o"
      },
      "source": [
        "# Step 9: Verify the environment\n",
        "Execute the two cell below and ensure that there are no runtime error and the rendering happens correctly. You should see output like this:\n",
        "\n",
        "```\n",
        "  âš½   +   ðŸ‘•    +  \n",
        "\n",
        "  +    +    +   ðŸ‘•  \n",
        "\n",
        "  +   ðŸ‘•    +    +  \n",
        "\n",
        "  +    +    +   ðŸ‘•  \n",
        "\n",
        "  +   ðŸ‘•    +    ðŸ¥…  \n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK5uEU1QGddR"
      },
      "source": [
        "foolsball = Foolsball(arena, agent, opponent, goal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nn8RNR1NDZK"
      },
      "source": [
        "foolsball.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjQffds2YGlj"
      },
      "source": [
        "# Step 10: Explore the environment\n",
        "- Run the next cell to play with the environment and score a few goals. \n",
        "- If there are any errors you may want to go back and update the code for the final implementation of the `Foolsball` class. \n",
        "- Make sure to run the cell containing your final implementation before retrying"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4w1HCRHwG60"
      },
      "source": [
        "## Move: n,s,e,w\n",
        "## Reset: r\n",
        "## Exit: x\n",
        "while True:\n",
        "    try:\n",
        "        act = input('>>')\n",
        "\n",
        "        if act in foolsball.actions:\n",
        "            print(foolsball.step(act))\n",
        "            print()\n",
        "            foolsball.render()\n",
        "        elif act == 'r':\n",
        "            print(foolsball.reset())\n",
        "            print()\n",
        "            foolsball.render()\n",
        "        elif act == 'x':\n",
        "            break\n",
        "        else:\n",
        "            print(f'Invalid input:{act}')\n",
        "    except Exception as e:\n",
        "        print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bBwFGPFe9Ld"
      },
      "source": [
        "# Step 12: Experiment with a different reward structure.\n",
        "- Does it encourage the agent to take the shortest route?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdzQu5qmStXR"
      },
      "source": [
        "## Different reward structure\n",
        "foolsball.set_rewards({'unmarked':0, 'opponent':-5, 'outside':-1, 'goal':+5})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyjTlB4cCysH"
      },
      "source": [
        "# Understanding the first bits of terminology.\n",
        "## State \n",
        "- In RL, state refers to information about the environemnt and then agent.\n",
        "- An RL algorithm inspects the state to decide which action to take.\n",
        "- Exactly what information gets captured in `state` depends on a few factors:\n",
        "  - The complexity of the environment: \n",
        "    - The number of actors, \n",
        "    - the nature of the environment, for example text or images. \n",
        "  - The complexity of the algorithm\n",
        "    - A simple algorithm may only need information about the agent and its immediate surroundings.\n",
        "    - A more complex algorithm may need information about the whole environment.\n",
        "\n",
        "\n",
        "## Setup\n",
        "- In our case we want the algorithm to only know about the location of the agent on the field. \n",
        "- We could have included information about the opponents too which would perhaps aid in the decision making but we chose not to.  \n",
        "\n",
        "- The state therefore is a tuple: (row, col), representing the location of the agent. \n",
        "- There are 20 possible values that `state` can take on:\n",
        "  - `row` can range from 0 through 4\n",
        "  - `col` can range from 0 through 3\n",
        "\n",
        "## Implementation details\n",
        "- The state is actually stored as a single integer that can take on values between 0 and 19."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5auCn-_GLVeU"
      },
      "source": [
        "## Actions\n",
        "The agents can perfrom actions in an environment.\n",
        "\n",
        "## Setup\n",
        "- Our agent can perform one kind of action: navigate up, down, right or left.\n",
        "- It has 4 actions: 'n', 'e', 'w', 's'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjxD0m0uMfbU"
      },
      "source": [
        "# Learning from experience\n",
        "Any RL set up can be modeled as shown below:\n",
        "\n",
        "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTMDmrmnl_dAyjCOErHPak2gLXmQTgQnVT8gQ&usqp=CAU)\n",
        "\n",
        "- The agent performs an action in the environment\n",
        "- The state of the environment and agent change as a result\n",
        "- The agent receives a reward and the updated state from the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASC5S75zN8mO"
      },
      "source": [
        "## Rewards\n",
        "- Reward is the signal that an agent receives after it performs an action.\n",
        "- The reward structure has to be decided by us. \n",
        "- The biggest challenge of RL is that reward is often sparse. \n",
        "\n",
        "## Set up\n",
        "- In our case the reward depends on the rules of the game and our goal.\n",
        "  - If the agent runs into an opponent, the game gets over and the reward is negative (penalizes the agent).\n",
        "  - If the agent makes it to the goalpost, the game gets over and the reward is positive.\n",
        "  - if the agent takes the ball out of the field the reward is negative.\n",
        "  - If the agent makes a valid move what shoud the reward be?\n",
        "\n",
        "## Implementation\n",
        "- The default reward structure in our case is  `{'unmarked':-1, 'opponent':-5, 'outside':-1, 'goal':+5}`\n",
        "- This can be changed at any time by calling `set_rewards()`.\n",
        "- Taking the ball to an unmarked position seems like a small step towards reaching the goalpost. Why would we then ever want to have a negative reward for this type of manouver?"
      ]
    }
  ]
}